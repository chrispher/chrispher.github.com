---
layout: post
title: Ask Me Anything_Yoshua Bengio
category: reddit
tags: [深度学习, 学习观点]
description: 介绍了Yoshua Bengio教授在深度学习和数据科学领域的一些问答和认知。
---

Yoshua Bengio教授是机器学习大神之一，尤其是在深度学习这个领域。他连同Geoff Hinton老先生以及 Yann LeCun（燕乐存）教授，缔造了2006年开始的深度学习复兴。他的研究工作主要聚焦在高级机器学习方面，致力于用其解决人工智能问题。他是仅存的几个仍然全身心投入在学术界的深度学习教授之一，好多其他教授早已投身于工业界，加入了谷歌或Facebook公司。作为机器学习社区的活跃者，Yoshua Bengio教授在美国东部时间2月27日下午一点到两点，在著名社区Reddit的机器学习板块参加了[“Ask Me AnyThing”](http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio/)活动，Yoshua回答了机器学习爱好者许多问题，干货频频。
注：翻译来自[Info](http://www.infoq.com/cn/articles/ask-yoshua-bengio-2)。

<!-- more -->

####问：
- 作为领域内的杰出教授，您个人如何看待当代神经网络忽如一夜又火了这个现象呢？您认为是理所应得还是夸大其词？还是两者兼有？或者您有完全不同的看法？还有，您对于当今文献对于神经网络研究的描述怎么看？ 我对于利用无监督技术来学习数据选择，以增加算法的普适能力这一块非常感兴趣。我感觉它是监督学习和非监督学习的有机结合，跟传统的预训炼不同。您所在的实验室已经在这方面取得了很多进展，使用“简单”的数据选择方法，比如高斯噪声选择法，即我们在DAE语境下所谓的输入dropout。您觉得这个方向算不算有潜力呢？希望您能给推荐一些相关资料，我找了很久都没找到。最后，没有人有水晶球来洞悉未来，但是您能谈谈您研究工作的下一步展望么？比如，过去几年基本是监督学习占了上风。

####答：
- 我觉得最近大家对于神经网络的极大热情，主要是因为机器学习同仁们浪费了多年的时间，1996到2006这20年，几乎从来没深入挖掘它。现在这个时候，确实有一些对于神经网络的夸大其词，尤其是在媒体上。这是非常不幸的，同时也非常危险，会被一些想要一夜暴富的公司所利用。危险尤其在于，一大波天花乱坠的许诺出来了，结果没有出色的实验结果作为根据。**科学在大部分时候都是小步前进的，我们必须要谦逊**。
- 我没有水晶球，但是我相信改进我们对于联合分布的建模，在未来深度学习的研究中，尤其是朝向人工智能级别的机器方面，非常重要，它能帮助机器更好的理解我们周围的世界。
- 另外一个比较容易预见的工作，是我们需要在训练高复杂度模型的过程中，找到快速有效的训练方法。不光是在训练模型本身（涉及到数值优化问题），而且在计算能力方面（比如通过并行或者其他的技巧来避免每个样本在训练的时候都牵扯到整个神经网络的更新）。你可以在arxiv上找到我的展望[文章](http://arxiv.org/abs/1305.0445)。

####问：
- 传统的（不管是不是深度）神经网络看起来在保留上下文信息方面有一些局限性。每个数据点/样本都是独立对待的。递归神经网络（Recurrent Neural Network，RNN）克服了这个问题，但是RNN训练起来非常难，而且一些RNN变种的尝试看起来也不是那么的成功。您觉得RNN在未来会流行起来么？如果是的话，在什么应用领域？以什么样的模型设计？非常感谢您百忙之中的回答！

####答：
- 回归网络或者叫做递归网络，在各种类型的对象的相互依赖关系的建模上，非常有用。我们小组在如何训练RNN上做了一些工作，这也是当前深度学习社区研究工作的重要组成部分。可能的应用领域有：语音识别（尤其是语言识别部分），机器翻译，情感分析，语音合成，手写合成与识别等等。

####问：
- 我很想听听您对liquid state machine（LSM）和深度学习之间的对比。

####答：
- liquid state machine和echo state networks（ESN）没有学习RNN里面类似的权重，换句话来说，它们不学习数据的表征。然而，深度学习最重要的目的，就是学习一个好的数据表征。从某种程度来讲，LSM之类的跟SVM很像，即给定一堆确定的特征，学习一个线性分类器。这里用到的特征是跟前面序列有关的函数，通过一些巧妙手段预先设置好权重。这些特征非常好，那么，能自动学习这些特征不是更好么！
- 网友附加答案：我觉得ESN和LSM非常好，学习它们能让我们更清楚RNN，了解RNN模型上什么情况下会给出很差结果，学好前两者对于学习DNN有好处。推荐看看Ilya在初始化方面的[工作](http://jmlr.org/proceedings/papers/v28/sutskever13.html)，文章阐述了采用Herbert Jaeger建议的ESN初始化方法的有效性，这对RNN同样奏效。另外LSM和ESN通常可以作为DNN很好的基准参考对比。还可以看看这个[页面](http://www.idsia.ch/~juergen/rnn.html)，很多RNN有用信息，尤其是LSTMNN，它是RNN变种之一，继绝RNN训练过程中梯度消失的问题，从而让RNN能够感知到更长的上下文。

####问：
- Bengio教授您好，我是McGill大学的本科生，从事类型论（译者注：与集合论差不多的学科）方面的研究，我的问题是：我所在的领域很注重形式化证明，机器学习领域有没有关注形式化证明的课题呢？如果没有的话，怎么保证学科的严谨性？有没有人研究用深度学习产生程序呢？我的直觉是最重我们可以用类型论来指定一个程序，并且用深度学习来搜索这个指定程序的实例，但现在我觉得可能还有很多困难。您能给举几个例子，关于机器学习中独特的数据结构的么？作为零起点的同学，我怎么才能开始深度学习呢？我不知道应该看一些什么资料，要是我能搞出点名堂，我非常愿意应聘您团队的研究职位。

####答：
- 有一种非常简单的方法，让我们无需证明，就能够得到学科的严谨性，大家都经常用到：叫做科学方法论，它依赖实验、假设和验证。另外，深度学习的论文里逐渐出现越来越多的数学。有一段时间，我曾对深度学习和浅层学习的特性对比很感兴趣（参见我和Delalleau或者更近的Pascanu一起合作的文章）。我还跟Nicolas Le Roux一起在RBM和DBN的近似特性上做了一点工作，我还建议你去看看Montufar的文章，很炫的数学。
- 至于零基础问题，有很多资料值得参考，比如deeplearning.net网站上等等。

####问：
- Bengio教授，在您的论文“Big Neural Networks Waste Capacity”中，您指出梯度下降法在神经元很多的时候没有少量神经元情况下好，我的问题是：增加的这些神经元和链接如何导致结果变坏的？您觉得类似（Martens 2010）提出的Hessian Free方法能否克服这个问题？("Deep learning via Hessian-free optimization." Proceedings of the 27th International Conference on Machine Learning (ICML-10). 2010)

####答：
- 增加的神经元和链接，其实引入了更多的曲率，即非对角海森矩阵。梯度下降法，作为一个一阶方法，会忽略这些曲率（它假设海森矩阵是单位矩阵）。所以神经元多了以后，梯度下降法就会在最小值附近跳来跳去，但总是不能有效的找到最小值。当然二阶方法也不是总有效果的。（译者注：可参考这篇[文章](http://www.cnblogs.com/tornadomeet/p/3267454.html)）

####问：
- 我来自蒙特利尔，一个创业公司，我对您的工作非常感兴趣，一个问题，貌似机器学习专家以及学术界对那些工业界的竞赛，比如Kaggle，不是很感兴趣啊。我知道获胜的概率确实比较低，让投入的时间和产出不成比例。而且很多机器学习爱好者都对此趋之若鹜，没有专家的参与感觉很受伤。一个机器学习领域的专家，难道不是几个小时就可以做出来一个比较不错的结果么？有没有这么一个场景，开放，协同，专家和爱好者一起工作的？

####答：
- 这有几个专家赢得Kaggle和Netflix的例子。机器学习专家不参与这种竞赛的原因，可能是他们那些好的解决办法，总是会有企业买单，不必参加类似的比赛来竞争。还有，专家从来都是乐于挑战极限的，而不是来面对日常生产环境里面那些非常令人烦躁的真实数据。参加这种竞赛，很大部分的时间都用来对数据进行预处理，而且，浅层模型如SVM、随机森林和boost方法很容易就能得出一个可接受的结果，这种做法没有什么学术价值。除了奖金方面，Kaggle这种竞赛的设置也是有问题的，可以参考这个非常有启发性的视频，大部分有能力可以独立思考的人都不会参与Kaggle。长话短说，竞赛只有能够彰显它在某个研究课题上的意义，才能吸引专家的参与。

####问：
- 我听说深度学习模型在训练过程中，很多地方都需要专家经验，手动调节，各种技巧，不知道有没有比较自动化的超参数学习方法呢？

####答：
- 超参数优化已经在深度学习领域中初见端倪，主要用在自动搜索模型的参数。所谓超参数，就是机器学习模型里面的框架参数，比如聚类方法里面类的个数，或者话题模型里面话题的个数等等，都称为超参数。它们跟训练过程中学习的参数（权重）是不一样的，通常是手工设定，不断试错调整，或者对一系列穷举出来的参数组合一通枚举（叫做网格搜索）。深度学习和神经网络模型，有很多这样的参数需要学习，这就是为什么过去这么多年从业者弃之不顾的原因。以前给人的印象，深度学习就是“黑魔法”。时至今日，非参数学习研究正在帮助深度学习更加自动的优化模型参数选择，当然有经验的专家仍然是必须的。
- 超参数的学习早已有之，但是直到最近才做出一点进展。这里面比较早期的主要贡献者（在应用到机器学习非参数学习领域之前）是Frank Hutter团队，他在2009年的博士论文就是关于软件系统里面如何用非参数学习来代替人手设定参数。我之前的博士生James Bergstra和我一起在这个问题上也研究过几年，我们提出了网格搜索的一种简单的取代方法，称作随机采样（[random sampling](http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)），实验结果非常好，也很容易实现。
- 随后我们就将Hutter在其他领域使用过的非参数学习方法引入了深度学习，称作序列优化（sequential optimization），发表在[NIPS 2011](http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)，我的另外一个联合培养博士生 Remi Bardenet和他的导师Balazs Kegl（前同事，现在法国）也参与了这个工作。这个工作被多伦多大学的研究人员看好并继续深入，其中有Jasper Snoek（Hinton教授的学生），Hugo Larochelle（我毕业的博士生）以及Ryan Adams（哈佛大学教授），他们的工作发表在[NIPS2012](http://www.dmi.usherb.ca/~larocheh/publications/gpopt_nips.pdf)。文中展示了他们利用自动化的方法，改进了Krizhevsky，Sutskever和Hinton教授非常著名的ImageNet物体识别神经网络算法，刷新了这个数据集的学术记录。
- Snoek等人开发了一个软件，被相关学者广泛使用，叫做spearmint，我最近发现Netflix在他们用深度学习做[电影推荐](http://techblog.netflix.com/2014/02/distributed-neural-networks-with-gpus.html)的新项目中也用到了它。
- 网友补充答案：补充一点贝叶斯优化以及[Hyperopt](http://hyperopt.github.io/hyperopt/)的相关内容，贝叶斯优化和专家参与相结合绝对是自动学习参数的好办法，参见[这个](http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio/cftfneh)和[ICML调试卷积神经网络](http://jmlr.org/proceedings/papers/v28/bergstra13.pdf)的内容。Hyperopt有个[Python库](https://github.com/hyperopt/hyperopt-sklearn)，提供ConvNets，NNets以及未来会涉及到机器学习库scikit-learn中一批分类器的自动化参数学习方法。

####问：
- 据我所知，您是机器学习领域唯一公开的以深度学习来研究社会学的科学家。在你那篇大作“[Culture vs Local Minima](http://arxiv.org/pdf/1203.2990v1.pdf)”中，您的阐述非常精彩，我有如下几个问题期待您的解答：

- 文章中您描述了个体是如何通过浸入社会来自学习的。众所周知，个体通常无法学到很多大局观念。如果您是这个世界的主宰，你有能力，设定一些观念，让所有个体从童年就开始学习，您会如何选择这些观念？
- “文化浸入”的一个必然结果，会让个体意识不到整个学习过程，对它来讲世界就是这个样子。作家David Foster Wallace曾经生动的将其比喻为“鱼需要知道水是什么”。在您的观点里，这种现象是神经网络结构的副产品还是它的确有一些益处？
- 您觉得文化趋势是否会影响个体并且导致它们赖在局部优化情况？比如各种宗教机构和启蒙哲学之间的争端，家长式社会和妇女参政之间的冲突。这种现象是有益还是有害的？
- 您对于冥想和认知空间如何看待？

####答：
- 我不是社会学或者哲学科学家，所以大家在看待我的回答的时候，需要用分析和辩证的眼光。我的看法是，非常多的个体固守自己的信念，因为这些信念已经变成了他们身份的一部分，代表了他们是怎么样的一个群体。改变信念是困难而且可怕的。我相信，我们大脑的很大一部分工作，就是试着让我们的所有经验和谐并存，从而形成一个良好的世界观。从数学的角度来讲，这个问题和推理（Inference）有关系，即个体透过观察到的数据，来寻找合适的解释（隐变量）。在随机模型里，推理过程通过一种给定配置的随机探索完成（比如马尔科夫网络是完全随机探索）。冥想之类的行为，从某种程度上帮助了我们提升推理能力。冥想的时候，有些想法灵光一现，而后我们发现它具有普适意义。这恰恰是科学进步的方法。

####问：
- 在讨论和积网络（sum product network，SPN）的时候，Google Brain小组的一个成员告诉我他对可计算的模型（tractable model）不感兴趣，您对此有何看法？

####答：
- 各种学习算法都不同程度地有很多不可计算性。通常来讲，越具有可计算性的模型的模型越简单，但是从表达能力上来讲就越弱。我并没有确切的计算过，和积网络将联合分布拆分之后，会损失多少计算能力。通常来讲，我所知道的模型都会受到不可计算性的影响（至少从理论上看，训练过程非常困难）。SVM之类的模型不会受到此类影响，但是如果你没有找到合适的特征空间，这些模型的普适性会受到影响。（寻找是非常困难的，深度学习正是解决了寻找特征空间的问题）。

- 网友补充：什么是**模型的可计算性**？就和积网络来讲，可计算性的意思就是，模型的推理能力在加入更多变量的时候，在计算要求上不会有指数级别的增加。可计算性是有代价的，和积网络只能表现某些特定的分布，详情可以参考Poon和Dmingo的论文。实际上，所有的图模型都能够表示成因子的乘积形式，深度信念网络也一样。图模型的推理，其可计算性主要取决于图的宽度（treewidth）。因此，低宽度的图模型被认为是可计算的，而高宽度则是不可计算的，人们需要使用MCMC、信念传播（BP）或者其他近似算法来寻求答案。任何的图模型网络，都可以转换成类似和积网络的形式（一种算数电路，AC）。问题在于，在极坏的情况下，转换生成的网络通常都是指数级别。所以，哪怕推理是跟网络规模线性相关的，在图模型大小增长的情况下，计算性也会呈指数下降。但是，值得一提的是，有一些指数级别的，或者说高宽度的图模型可以被转换成紧致（compact）算数电路，使得我们仍然可以在其上进行推理，即可计算，这个发现曾经让图模型社区非常振奋。我们可以把AC和SPN理解成一种紧致的表示图模型上下文无关的方式。它们能够将一些高宽度的图模型表示成紧致形式。AC和SPN的区别在于，AC是通过贝叶思网络转换而来，SPN则是直接表示概率分布。所以，取代传统图模型的训练，我们可以将其转换成紧致电路（AC），或者学出来一个紧致电路（SPN）。

####问：
- 为什么深度网络会比浅层网络效果更好？众所周知，有一个隐含层的网络实际上是一个全局逼近器，添加更多全联通层次通常会改进效果，这种情况有没有理论依据呢？我所接触到的论文都声称确实改进了效果，但是都语焉不详。在您没有发表的想法里面，您最中意哪一个？您曾经审阅过的最可笑或者最奇怪的论文是什么？如果我没弄错的话，您用法语授课，这是个人爱好还是学校的要求？

####答：
- **全局逼近器并不会告诉你需要多少个隐含层。对于不确定的函数，增加深度并不会改进效果。然而，如果函数能够拆分成变量组合的形式，深度能够起到很大作用，无论从统计意义（参数少所需训练数据就少）来讲，还是从计算意义（参数少，计算量小）来讲**。
我用法语教书是因为Montreal大学的官方语言是法语。不过我的毕业生里面四分之三都不是以法语为主要语言的，感觉没什么影响。关于在Montreal生活，我的学生写了一个生活描述，提供给申请的同学们[参考](http://www.iro.umontreal.ca/~bengioy/yoshua_en/index_files/open_positions.html)。Montreal 是个很大的城市，有四所大学，非常浓厚的文化氛围，贴近自然，生活质量（包括安全）全北美排第四。生活成本相对其他类似城市也低很多。

####问：
- 众所周知，深度学习已经在图像、视频和声音上取得了突破，您觉得它能否在文本分类上也会取得进展呢？大部分用于文本分类的深度学习，结果看起来跟传统的SVM和贝叶思相差无几，您怎么认为？

####答：
- 我预感深度学习肯定会在自然语言处理方面产生非常大的影响。实际上影响已经产生了，跟我在NIPS 2000年和JMLR 2003年的论文有关：用一个学习出来的属性向量来表示单词，从而能够对自然语言文本中单词序列的概率分布建模。目前的工作主要在于学习单词、短语和句子序列的概率分布。可以看一看Richard Socher的工作，非常的深入。也可以看看Tomas Mikolov的工作，他用递归神经网络击败了语言模型的世界纪录，他研究出来的分布，在一定程度上揭示了单词之间某些非线性的关系。例如，如果你用“意大利”这个单词的属性向量来减去“罗马”的属性向量，再加上“巴黎”的属性向量，你能得到“法国”这个单词或者相近的意思。类似的，用“国王”减去“男人”加上“女人”，能得到“王后”。这非常令人振奋，因为他的模型并没有刻意的设计来做这么一件事。

####问：
- 我看到越来越多的杂志报道深度学习，称之为通往真正人工智能（AI）的必经之路，连线杂志是“罪魁祸首”啊。鉴于人工智能在七八十年代的低潮（当时的人们也是对此期望颇高），您觉得深度学习和机器学习研究者应该做一些什么来防止类似再次发生呢？

####答：
- 我的看法是，还是**要以科学的方式来展示研究进展（就这一点，很多标榜自己从事深度研究的公司都做不到）。别过度包装，要谦虚，不能将目前取得的成绩过度消费，而是立足一个长远的愿景。**


####问：
- 首先您实验室开发的theano和pylearn2非常赞。四个问题：1:您对于Hinton和Lecun转战工业界啥看法？
2:比起私人公司里闷头赚大钱，您觉得学术研究和发表论文的价值在于？3:您觉得机器学习会不会变得和时间序列分析领域一样，很多研究都是封闭的，各种知识产权限制？4:鉴于目前判别式神经网络模型取得的进展，您觉得产生式模型未来能有什么发展？

####答：
- 我觉得Hinton和Lecun投身工业界，会带动更多更好的工业级神经网络应用，来解决真正有趣的大规模问题。遗憾的是深度学习领域可能短期少掉很多给博士申请同学们的offer。当然，深度研究领域的第一线还是有很多成长起来的年轻研究者，很愿意招收有能力的新同学。深度学习在工业界的深入应用，会带动更多的同学了解和理解这个领域，并投身其中。个人来讲，我喜欢学术界的自由，而非给薪水上多加几个零。我觉得就论文发表来讲，学术界会持续产出，工业界的研究所也会保持热情高涨。产生式模型未来会变得很重要。你可以参考我和Guillaume Alain关于非监督学习方面的文章（注意这两者并不是同义词，但是通常会一起出现，尤其是我们发现了自动编码器（auto-encoder）的产生式解释之后）。

####问：
- 在您工作的启发下，我去年用概率模型和神经网络完成了关于自然语言处理（NLP）的本科论文。当时我对此非常感兴趣，决定从事相关领域的研究，目前我在攻读研究生，还听了一些相关课程。但是，过了几个月，我发现NLP并没有我想象的那么有意思。这个领域的研究人员都有那么一点迟钝和停滞，当然这是我的个人片面看法。您觉得NLP领域的挑战是什么？ 

####答：
- 我相信，NLP里面真正有意思的挑战，即“自然语言理解”的关键问题，是如何设计学习算法来表示语意。例如，我现在正在研究给单词序列建模的方法（语言模型）或者将一个语言里的一句话翻译成另一个语言里同样意思的一句话。这两种情况，我们都是在尝试学习短语或者句子的表示（不仅仅是一个单词）。就翻译这个情况来讲，你可以把它当成一个自动编码器：编码器（比如针对法语）将一句法语句子映射到它的语意表示（用一个通用方法表示），另一个解码器（比如针对英语），可以将这个表示依照概率分布映射到一些英文句子上，这些句子跟原句都有一样或者近似的语意。同样的方法，我们显然可以应用到文本理解，稍微加上一点额外的工作，我们就可以做自动问答之类的标准自然语言处理任务。目前我们还没有达到这个水平，主要的挑战我认为存在于数值优化部分（训练数据量大的时候，神经网络很难训练充分）。此外，计算方面也存在挑战：我们需要训练更大模型（比如增大一万倍）的能力，而且我们显然不能容忍训练时间也变成一万倍。并行化并不简单，但是会有所帮助。目前的情况来讲，还不足以得到真正好的自然语言理解能力。好的自然语言理解，能通过一些图灵测试，并且需要计算机理解世界运行所需要的很多知识。因此我们需要训练不光仅仅考虑了文本的模型。单词序列的语意可以同图像或者视频的语意表示相结合。如上所述，你可以把这个结合过程认为是从一个模态向另一个模态的转化，或者比较两个模态的语意是否相似。这是目前Google图片搜索的工作原理。 

####问：
- 我正在写本科论文，关于科学和逻辑的哲学方面。未来我想转到计算机系读硕士，然后攻读机器学习博士学位。除了恶补数学和编程以外，您觉得像我这样的人还需要做些什么来吸引教授的目光呢？

####答：
- 阅读深度学习论文和教程，从介绍性的文字开始，逐渐提高难度。记录阅读心得，定期总结所学知识。
把学到的算法自己实现一下，从零开始，保证你理解了其中的数学。别光照着论文里看到的伪代码复制一遍，实现一些变种。用真实数据来测试这些算法，可以参加Kaggle竞赛。通过接触数据，你能学到很多；
把你整个过程中的心得和结果写在博客上，跟领域内的专家联系，问问他们是否愿意接收你在他们的项目上远程合作，或者找一个实习。找个深度学习实验室，申请；这就是我建议的路线图，不知道是否足够清楚？

####问：
- 教授您好，蓝脑项目组的研究人员试图通过对人脑的逆向工程来建造一个能思考的大脑。我听说Hinton教授在某次演讲的时候抨击了这个想法。这给了我一个印象，Hinton教授觉得机器学习领域的方法才更可能造就一个真正的通用人工智能。让我们来假想一下未来的某一个时候，我们已经创造出了真正的人工智能，通过了图灵测试，它活着并且有意识。如果我们能看到它的后台代码，您觉得是人脑逆向工程造就了它，还是人造的成分居多？

####答：
- 我不认为Hinton教授实在抨击人脑逆向工程本身，即他并不反对从人脑中学习如何构建智能机器。我猜测他可能是对项目本身的质疑，即一味的尝试拿到更多大脑的生理细节，而没有一个全局的计算理论来解释人脑中的计算是如何进行和生效的（尤其是从机器学习的角度）。我记得他曾经做过这么一个比喻：想象一下我们把汽车所有的细节都原封不动的复制过来，插上钥匙，就期待汽车能够在路上自己前进，这根本就不会成功。我们必须知道这些细节的意义是什么。

####问：
- 有没有人将深度学习应用到机器翻译中呢？您觉得基于神经网络的方法，什么时候才能在商业机器翻译系统中取代基于概率的方法呢？ 

####答：
- 我刚开了一个[文档](https://docs.google.com/document/d/1lqo5N1LzVWNPy1sYuujNa5vVNmyP5Zjv6VtEVgcFr6k)，罗列一些机器翻译方面的神经网络论文。简单来说，由于神经网络已经从语言模型上胜出了n-grams，你可以首先用它们来替代机器翻译的语言模型部分。然后你可以用它们来代替翻译表（毕竟它只是另一个条件概率表）。很多有意思的工作都正在开展。最宏大和让人兴奋的是完全摒弃现在的机器翻译流水线方法，直接用深度模型从头到尾学习一个翻译模型。这里有意思的地方在于，输出结果是结构化的（是一个单词序列的联合分布），而不简单的是一个点预测（因为对于一个原句来说，有很多翻译的可能性）。
- 网有补充资料：纽约时报有一篇[文章](http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html)谈到了从英语到普通话的，微软出品。

####问：
- 教授您好，我在各种项目里应用最多的还是决策树和随机森林。您能给讲讲深度学习对比而来的好处么？

####答：
- 我曾经写过一篇[文章](http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+al-decisiontrees-2010.pdf)，阐述为什么决策树的普适性比较差。这里面的核心问题是，决策树（以及其他机器学习算法）将输入空间划分，而后每个区域分配独立的参数。因此对于新的区域以及跨区域的情况，算法的效果就会变差。你没办法学到这么一个函数，能够覆盖比训练数据要多的独立区域。神经网络没有这个问题，具有全局特性，因为它的参数可以被多个区域公用。

####问：
- 在深度学习领域，您有什么好书或者论文推荐？

####答：
- 好文章太多了，我们组内有一个给新同学的[阅读列表](https://docs.google.com/document/d/1IXF3h0RU5zz4ukmTrVKVotPQypChscNGf5k6E25HGvA)。

####问：
- 今日的机器学习技术是否会成为明日人工智能的基石？人工智能发展的最大困难在哪里？是硬件还是软件算法的问题？您对于Ray Kurzweil'预言2029年机器会通过图灵测试怎么看? 他还写了一篇打赌的[文章](http://www.kurzweilai.net/a-wager-on-the-turing-test-the-rules)呢。

####答：
- 我不敢说2029年机器会通过图灵测试，但是我能确定的是，机器学习会成为研发未来人工智能的核心技术。
人工智能发展的最大问题，是改进机器学习算法。要想得到足够好的机器学习算法，有很多困难，比如计算能力，比如概念理解上的。比如学习一些联合概率。我觉得我们在训练超大规模神经网络的优化问题上，还是浮于表面。接着就是增强学习，非常有用，亟待改善。可以参看一下最近DeepMind公司的工作，他们用神经网络来自动进行八十年代的Atari游戏，非常有意思。文章发表在我组织的NIPS的讨论会上。

####问：
- 您对Jeff Hawkins对深度学习的批评有什么看法？Hawkins是On Intelligence一书的作者, 该书2004年出版，内容关于大脑如何工作，以及如何参考大脑来制造智能机器。他声称深度学习没有对时间序列建模。人脑是基于一系列的传感数据进行思考的，人的学习主要在于对序列模式的记忆，比如你看到一个搞怪猫的视频，实际是猫的动作让你发笑，而不是像Google公司所用的静态图片。参见这个[链接](http://www.technologyreview.com/featuredstory/513696/deep-learning/)

####答：
- 时间相关的神经网络其实有很多工作，递归神经网络模型对时间关系隐性建模，通常应用于语音识别。比如下面这两个工作。
- [1] http://www.cs.toronto.edu/~hinton/absps/RNN13.pdf
- [2] http://papers.nips.cc/paper/5166-training-and-analysing-deep-recurrent-neural-networks.pdf
- 还有这篇文章：http://arxiv.org/abs/1312.6026. 自然语言处理中的序列也有所考虑：http://arxiv.org/abs/1306.2795

####问：
- 深度学习到底在什么领域很有前途？什么领域是它的弱项呢？为什么栈式RBM效果很好？其原理能否解释清楚？还是仍然类似魔术黑箱一样？聚合学习和深度学习之间有何联系？

####答：
- 完全不是魔术黑箱。我相信我已经给出了栈式RBM或者自动编码器为何有效的解释。参见我和Courville 以及Vincent的文章：http://arxiv.org/abs/1206.5538 除了dropout技术的解释以外，我不知道聚合学习和深度学习之间的关系，可以参考这篇文章： http://arxiv.org/abs/1312.6197

####问：
- 根据我的理解，深度神经网络训练上的成功跟选取正确的超参数有关系，比如网络深度，隐含层的大小，稀疏约束值等等。有些论文基于随机搜索来寻找这些参数。可能跟代码写得好也有关系。有没有一个地方能让研究者找到某些特定任务的合理超参数呢？在这些参数的基础上，可能更容易找到更优化的参数。

####答：
- 可以看上文关于超参数的部分。James Bergstra 继续了这部分工作。我觉得有这么一个数据库，存储着许多推荐的超参数设置，对于神经网络训练是非常有好处的。Github上面的Hyperopt项目，做了类似的事情。hyperopt项目聚焦于神经网络、卷积网络，给出一些超参数设置的建议。以简单的因子分布的形式给出。比如隐含层的数量应该是1到3，每一层的隐含单元数目应该是50到5000。其实超参数还有很多，以及更好的超参数搜索算法等等。下面是更多的参考论文：
- http://arxiv.org/abs/1306.2795
- http://arxiv.org/abs/1312.6026
- http://arxiv.org/abs/1308.0850
- http://papers.nips.cc/paper/5166-training-and-analysing-deep-recurrent-neural-networks.pdf

####问：
- 有没有什么应用，传统机器学习方法都失败了，而深度学习成功了？

####答：
- 有一个构造出来的应用，由两个简单的任务构成（物体检测，逻辑推理），该应用聚焦于隐变量的内在表示，传统黑盒机器学习算法都失败了，有一些深度学习算法结果还不错，但也有深度学习算法失败了。可以看看这篇[文章](http://arxiv.org/abs/1301.4083)。这个应用有意思的地方在于它比那两个任务随便一个都复杂得多。

####问：
- Bengio教授，在深度学习中，有那么一类方法，采用比较高级的数学如代数和拓扑集合。John Healy几年前声称通过通过范畴论（Category Theory）改进了神经网络（ART1）。您对于这类尝试有什么看法？是儿戏还是很有前途？

####答：
- 可以看看Morton和Montufar的工作，参考附加材料：
- http://www.ece.unm.edu/~mjhealy/
- http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.98.6807
- 热带几何以及概率模型中的热带几何
- http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.242.9890

####问：
- Bengio教授，我即将完成计算神经学的博士，我对于神经科学和机器学习交叉产生的“灰色地带”非常感兴趣。您觉得脑科学的那些部分和机器学习有关？您想要了解脑科学的什么方面？

####答：
- 我认为，理解大脑的计算过程跟机器学习强相关。我们尚未知晓大脑的工作机制，它的高效学习模式会对我们设计和实现人工神经网络有很大的指导意义，所以这个是非常重要的，也是机器学习领域和脑科学的交叉区域。